---
# ADMIN CREDENTIALS
# =================
#
# WARNING: It is strongly recommended to store these settings in Ansible vault
# file named "secret" under override files directory. Configuration parameters
# stored in vault must start with vault_ prefix (i.e. vault_admin_username,
# vault_admin_password).
#
admin_username: admin
admin_password: St8rlingX*

# INITIAL PASSWORD CHANGE RESPONSE SEQUENCE
# =========================================
#
# The following two parameters are only relevant when the target host is bootstrapped
# remotely and the user wishes to change the initial sysadmin password as part of the
# bootstrap.
#
# WARNING: It is strongly recommended to store this setting in Ansible vault
# file named "secret" under override files directory. Configuration parameters
# stored in vault must start with vault_ prefix (i.e. vault_password_change_responses)
#
password_change: false

password_change_responses:
  yes/no: 'yes'
  sysadmin*: 'sysadmin'
  \(current\) UNIX password: 'sysadmin'
  (?i)New password: 'St8rlingX*'
  (?i)Retype new password: 'St8rlingX*'

# OVERRIDE FILES DIRECTORY
# ========================
#
# Default directory where user override file(s) can be found
#
override_files_dir: "{{ lookup('env', 'HOME') }}"

# BACKUP AND RESTORE
# ==================
#
# Location where the backup tar file is placed to perform the restore.
# This location must be specified at the command line via ansible-playbook -e option.
initial_backup_dir:

# This variable refers to the tar file that is generated by the backup
# procedure and used in the restore phase. The filename must be specified
# at the command line via ansible-playbook -e option.
backup_filename:

# Default directory where the backup tar file(s) can be found
# on the active controller
backup_dir: /opt/backups

# The platform backup tarball will be named in this format:
# <platform_backup_filename_prefix>_<timestamp>.tgz
#
platform_backup_filename_prefix: "{{ inventory_hostname }}_platform_backup"

# The local registry images backup tarball will be named in this format:
# <docker_local_registry_backup_filename_prefix>_<timestamp>.tgz
#
docker_local_registry_backup_filename_prefix: "{{ inventory_hostname }}_docker_local_registry_backup"

# The stx-openstack application backup tarball will be named in this format:
# <openstack_backup_filename_prefix>_<timestamp>.tgz
#
openstack_backup_filename_prefix: "{{ inventory_hostname }}_openstack_backup"

# An indication whether it is a full restore or partial restore.
#   true:  a full restore where storage partition(s) is/are wiped during
#          platform restore and cinder+glance data needs restored
#   false: a partial restore where ceph data remain intact during restore
#
# This variable is used for StarlingX OpenStack application restore only
#

# The dc_vault backup tarball will be named in this format:
# <dc_vault_backup_filename_prefix>_<timestamp>.tgz
#
dc_vault_backup_filename_prefix: "{{ inventory_hostname }}_dc_vault_backup"

restore_cinder_glance_data: false

# Default directory where the system backup tarballs fetched from the
# active controller can be found
#
host_backup_dir: "{{ lookup('env', 'HOME') }}"

# Flag file to indicate if platform restore is in progress
#
restore_in_progress_flag: /etc/platform/.restore_in_progress

# This variable is set to true when restore_openstack playbook is
# played again to bring up remaining OpenStack services after Ceph
# data is restored.
restore_openstack_continue: false

# When set to false, disk partitions that were previously used for Ceph data are
# not wiped. Otherwise, all disks are wiped as part of the bootstrap.
wipe_ceph_osds: false

# The following parameter indicates where the backup data file(s) reside,
# on the host itself (true) or off box (false).
on_box_data: true
