---
#
# Copyright (c) 2023 Wind River Systems, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# SUB-TASK DESCRIPTION:
#   These tasks recover the kubernetes leaf certificates after expiry
#   and verify the k8s cluster is healthy afterwards
#
- name: Recover k8s control plane leaf certificate on controller nodes
  block:
  - name: Verify k8s leaf certificates expiration
    shell: |
      K8_OUT=$(kubeadm certs check-expiration --config /etc/kubernetes/kubeadm.yaml)
      if [ "$?" -eq "0" ]; then
          echo "$K8_OUT"
          EXPIRED_K8S_CERTS=$(echo "$K8_OUT" | grep invalid)
          if [ "$EXPIRED_K8S_CERTS" -ne "0" ]; then
              exit 0
          else
              exit 1
          fi
      else
          exit 1
      fi
    register: k8s_certs_expiration
    failed_when: false
    become: yes

  - name: Recover k8s control plane leaf certificates
    command: bash -x /usr/bin/kube-cert-rotation.sh
    register: kube_cert_rotation_out
    become: yes
    retries: 5
    delay: 10
    until: kube_cert_rotation_out is not failed
    when: k8s_certs_expiration.rc != 0

  when: is_controller

- name: Renew kubelet leaf certificates
  import_tasks: recover-kubelet-certificates.yml

- name: Restart K8s cluster after recovery on the active controller
  block:
  - name: Wait till kubectl starts replying
    shell: kubectl get node $(hostname)
    environment:
      KUBECONFIG: "/etc/kubernetes/admin.conf"
    register: k8s_get_nodes
    # Waits up to 2 minutes, but in a freshly installed system, it takes ~ 20secs
    retries: 12
    delay: 10
    until: k8s_get_nodes is not failed

  - name: Trigger restart of networking pods first to avoid pod scheduling issues
    shell: |
      kubectl -n kube-system delete pods -l app=multus --wait=false
      kubectl -n kube-system delete pods -l k8s-app=kube-proxy --wait=false
    environment:
      KUBECONFIG: "/etc/kubernetes/admin.conf"

  # todo (rjosemat) replace this with a dynamic wait instead
  - name: Pause for 10 seconds to wait k8s to start rolling out pods
    pause:
      seconds: 10

  - name: Trigger a restart of every pod (deployment,statefulset,daemonset rollout)
    shell: >-
      kubectl get deployment,statefulset,daemonset -A --no-headers |
      awk '{ print  " rollout restart " $2 " -n " $1}' | xargs -n5 kubectl;
    environment:
      KUBECONFIG: "/etc/kubernetes/admin.conf"

    # Waits for pods to become READY for the current controller.
    # Calico-node is excluded because it only becomes READY when both kubelet nodes are recovered,
    # it waits for the BGP peer of the other calico node in controller-1 to be available.
  - name: Wait pods to restart (become READY) on controller
    shell: >-
      kubectl get po -l '!job-name' -A --no-headers -o
      'custom-columns=NAME:.metadata.name,
      READY:.status.containerStatuses[*].ready,NODE:.spec.nodeName'
      | grep -v calico-node
      | grep $(hostname)
      | grep -cv true
    environment:
      KUBECONFIG: "/etc/kubernetes/admin.conf"
    register: pods_starting
    failed_when: pods_starting.stdout | int != 0
    # Waits for up to one hour. This is the same value used in k8s-rootca-update
    # A large number is used because the recovery time depends on the number of pods running.
    # In a freshly installed system, without additional pods, it takes ~ 1min 10secs
    retries: 360
    delay: 10
    until: pods_starting.stdout | int == 0

  - name: Set fact to mark that K8s leaf certificates recovery was performed
    set_fact:
      subcloud_k8s_leaf_certs_recovered: true

  when:
    - is_controller
    - k8s_certs_expiration.rc != 0 or kubelet_certs_expiration.rc != 0
