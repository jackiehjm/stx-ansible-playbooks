---
#
# Copyright (c) 2019-2022 Wind River Systems, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# ROLE DESCRIPTION:
#   This role is to backup StarlingX platform data and
#   StarlingX OpenStack Application data if the app exists.
#   The backup data are stored in two separate tar files.
#

- name: Normalize/Set default parameters
  set_fact:
    # Set parameters for ldap different paths by OS
    ldap_schema_path: "{{ '/etc/openldap/schema' if os_release == 'centos' else '/etc/ldap/schema' }}"
    backup_registry_filesystem_required: "{{ backup_registry_filesystem | bool }}"
    should_use_old_image_backup: "{{ backup_user_local_registry|bool == true }}"


- name: Do StarlingX backup
  block:
    - name: Check if pigz package is installed
      block:

        - name: Issue command to pkg manager
          command: "{{ 'rpm -q' if os_release == 'centos' else 'dpkg -l' }} pigz"
          args:
            warn: false
          failed_when: false
          register: check

        - set_fact:
            pigz_check: "{{ 'succeeded' if check.rc == 0 else 'failed' }}"

      when: os_release in ["centos", "debian"]

    - name: Check if pigz package is installed
      package:
        name: pigz
        state: present
      check_mode: true
      register: pigz_check
      when: os_release not in ["centos", "debian"]

    - name: Check number of platform cores
      shell: |
        source /etc/platform/openrc
        system host-cpu-list $(hostname) --nowrap | grep " Platform " | wc -l
      register: num_platform_cores

    - name: Set compress program for backup tarball
      set_fact:
        compress_program: "{{ 'pigz' if num_platform_cores.stdout | int >= 4 and pigz_check is succeeded else 'gzip' }}"

    - name: Get kube-system default-registry-key
      command: >-
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get secret default-registry-key --namespace=kube-system
      failed_when: false
      register: kube_system_default_registry_key

    - name: Fail if there is no default-registry-key for kube-system
      fail:
        msg: "default-registry-key not found. Platform backup cannot proceed without it."
      when: kube_system_default_registry_key.rc != 0

    - name: Send application lifecycle notifications for pre-backup semantic check
      command: /usr/bin/sysinv-utils notify backup-semantic-check
      register: backup_semantic_check_notification_result
      failed_when: false

    - name: Fail if some application won't allow backup to proceed because semantic check failed.
      fail:
        msg: >
          Semantic check failed for backup action from application
          {{ backup_semantic_check_notification_result.stderr }}.
      when: backup_semantic_check_notification_result.rc == 1

    - name: Fail if there is some other/internal error when sending lifecycle hook.
      fail:
        msg: "Failed to run backup-semantic-check action."
      when: backup_semantic_check_notification_result.rc == 2

    - name: Send application lifecycle notifications for pre-backup action
      command: /usr/bin/sysinv-utils notify pre-backup-action
      register: pre_backup_notification_result
      failed_when: false

    - name: Fail if some application cannot handle the pre-backup action
      fail:
        msg: >
          Pre-backup action for application
          {{ pre_backup_notification_result.stderr }}.
      when: pre_backup_notification_result.rc != 0

    - name: Generate backup_in_progress alarm
      script: fm_alarm.py "--set" "--backup"
      register: alarm_result
      failed_when: false

    - name: Fail if alarm script throws an exception
      fail:
        msg: "Failed to generate backup-in-progress alarm."
      when: alarm_result.rc != 0

    - name: Create temp dir
      tempfile:
        path: "{{ backup_dir }}"
        state: directory
      register: tempdir

    - name: Create postgres temp dir
      file:
        path: "{{ tempdir.path }}/postgres"
        state: directory
      register: postgres_dir

    - name: Backup roles, table spaces and schemas for databases.
      shell: >-
        sudo -u postgres pg_dumpall
        --clean --schema-only > {{ postgres_dir.path }}/postgres.postgreSql.config
      args:
        warn: false

    - name: Backup postgres, template1, sysinv, barbican, helmv2 db data
      shell: >-
        sudo -u postgres pg_dump --format=plain --inserts --disable-triggers --data-only
        {{ item }} > {{ postgres_dir.path }}/{{ item }}.postgreSql.data
      args:
        warn: false
      with_items:
        - postgres
        - template1
        - sysinv
        - barbican
        - helmv2

    - name: Backup fm db data
      shell: >-
        sudo -u postgres pg_dump --format=plain --inserts --disable-triggers
        --data-only fm --exclude-table=alarm > {{ postgres_dir.path }}/fm.postgreSql.data
      args:
        warn: false

    - name: Backup keystone db data
      shell: >-
        sudo -u postgres pg_dump --format=plain --inserts --disable-triggers
        --data-only keystone > {{ postgres_dir.path }}/keystone.postgreSql.data
      args:
        warn: false

    - name: Check if it is dc controller
      command: >-
        grep -i "distributed_cloud_role\s*=\s*systemcontroller"
        {{ platform_conf_path }}/platform.conf
      register: check_dc_controller
      failed_when: false

    - block:
      - name: Backup dcmanager db for dc controller
        shell: >-
          sudo -u postgres pg_dump --format=plain --inserts --disable-triggers
          --data-only dcmanager > {{ postgres_dir.path }}/dcmanager.postgreSql.data
        args:
          warn: false

      - name: Backup dcorch db for dc controller
        set_fact:
          dcorch_db: "sudo -u postgres pg_dump --format=plain --inserts --disable-triggers --data-only dcorch "

      - name: Update dcorch tables that will be excluded from backup
        set_fact:
          dcorch_db: "{{ dcorch_db }} --exclude-table={{ item }}"
        with_items:
          - orch_job
          - orch_request
          - resource
          - subcloud_resource

      - name: Backup dcorch db
        shell: "{{ dcorch_db }} > {{ postgres_dir.path }}/dcorch.postgreSql.data"

      when: check_dc_controller.rc == 0

    - name: Create mariadb temp dir
      file:
        path: "{{ tempdir.path }}/mariadb"
        state: directory
      register: mariadb_dir

    - name: Check if mariadb pod is running
      shell: >-
        kubectl --kubeconfig={{ kube_config_dir }} get pods -n openstack | grep {{ mariadb_pod }} | grep -i 'running'
      failed_when: false
      register: check_mariadb_pod

    - block:
      - name: Set k8s cmd prefix
        set_fact:
          kube_cmd_prefix: "kubectl --kubeconfig={{ kube_config_dir }} exec -i {{ mariadb_pod }} -n openstack -- bash -c "

      - name: Show databases
        shell: "{{ kube_cmd_prefix }} 'exec mysql -uroot -p\"$MYSQL_DBADMIN_PASSWORD\" -e\"show databases\"'"
        register: databases

      - name: Backup mariadb
        shell: >-
          {{ kube_cmd_prefix }} 'exec mysqldump -uroot -p"$MYSQL_DBADMIN_PASSWORD" {{ item }}' >
          {{ mariadb_dir.path }}/{{ item }}.mariadb.data
        with_items: "{{ databases.stdout_lines | difference(skip_os_dbs) }}"

      when: check_mariadb_pod.rc == 0

    - block:

      - name: Get openstack application status
        shell: >-
          source /etc/platform/openrc; system application-show {{ openstack_app_name }} --column status --format value
        failed_when: false
        register: openstack_status

      - name: Fail the backup if MariaDB is not running
        fail:
          msg: "WARNING: {{ mariadb_pod }} is not running. Cannot backup mariadb data."
        when: openstack_status.stdout == "applied"
      when: check_mariadb_pod.rc != 0

    # In order to restore only the OpenStack application, the user needs to manually
    # remove, delete and upload the application.
    # This procedure will make the helm overrides data that is
    # present in the sysinv DB and the OpenStack application DB to be inconsistent.
    # A dump file containing the overrides data is used here to solve this inconsistency issue.
    # Note that this file will contain only 'update' sql commands because the
    # overrides are already created when we uploaded the application.
    - name: Create Helm overrides temp dir
      file:
        path: "{{ tempdir.path }}/helm_overrides_sqldump_dir"
        state: directory
      register: helm_overrides_sqldump_dir

    - name: Get the openstack Helm overrides from the from the database
      shell: >-
        psql -c "copy(select row_to_json(t) from (select name, user_overrides, system_overrides
        from helm_overrides where namespace='openstack') as t) to stdout" sysinv | sed -e 's/\\\\/\\/g'
      become_user: postgres
      register: helm_overrides_list

    - name: Generate postgres update commands for Helm overrides
      set_fact:
        updates_list: >
          {{ updates_list | default('') }}update helm_overrides set
          system_overrides={% if item.system_overrides %}'{{ item.system_overrides }}'{% else %}NULL{% endif %},
          user_overrides={% if item.user_overrides %}'{{ item.user_overrides }}'{% else %}NULL{% endif %}
          where name='{{ item.name }}' and namespace='openstack';
      with_items: "{{ helm_overrides_list.stdout_lines | map('from_json') | list }}"

    - name: Backup Helm overrides
      copy:
        dest: "{{ helm_overrides_sqldump_dir.path }}/helm_overrides_dump.sql"
        mode: 0755
        content: '{{ updates_list | default("") }}'

    - name: Set exclude targets
      set_fact:
        exclude_targets:
          "{{ backup.exclude + _exclude_dirs.split(',') | reject('equalto', '') | list }}"
      vars:
        _exclude_dirs: "{{ exclude_dirs | default(\"\") }}"

    # Now Postgres data and MariaDB data are stored in staging dir, we can estimate
    # the disk size requirement for the backup archive.
    - name: Check the size (in KiB) of directories that will be backed up for platform
      shell: "du -sh -k  {{ item }} --exclude {{ exclude_targets|join(' --exclude ') }} | awk '{print $1}'"
      register: size_output_platform
      with_items: "{{ backup.targets }}"

    # Estimate the backup size. We add 128M overhead for things like ceph crushmap,
    # ldap data, etc. that will be generated and stored in the staging dir later on.
    - name: Estimate the total required disk size for platform backup archive
      set_fact:
        total_platform_size_estimation: "{{ total_platform_size_estimation|default(1024*128)|int + item.stdout|int }}"
      with_items: "{{ size_output_platform.results }}"
      loop_control:
        label: "{{ item.item }}"

    # Estimate registry filesystem backup size
    - name: Verify image registry filesystem backup size
      block:

        - name: Get the size of each image directories to be backed up (in KiB)
          shell: "du -sh -k  {{ item }} | awk '{print $1}'"
          register: size_output_registry_image_fs
          with_items: "{{ image_backup.targets }}"

        - name: Determine the free disk space requirement for image registry backup
          set_fact:
            total_platform_size_estimation: "{{ total_platform_size_estimation|default(1024*128)|int + item.stdout|int }}"
          with_items: "{{ size_output_registry_image_fs.results }}"
          loop_control:
            label: "{{ item.item }}"

      when: backup_registry_filesystem_required

    # For SystemController the dc-vault is part of platform but restored after controller-0 unlock
    # Create a separate archive for it
    - block:
        - name: Check the size (in KiB) of directories that will be backed up for dc-vault
          shell: "du -sh -k  {{ dc_vault_permdir }} | awk '{print $1}'"
          register: size_output_dc_vault

        - name: Estimate the total required disk size for platform backup archive
          set_fact:
            total_platform_size_estimation: "{{ total_platform_size_estimation|int + size_output_dc_vault.stdout|int }}"
      when: check_dc_controller.rc == 0

    - name: Check the free space in the archive dir
      shell: "df -k {{ backup_dir }} --output=avail | tail -1"
      register: df_output

    - name: Parse backup directory size
      set_fact:
        available_disk_size: "{{ df_output.stdout | int }}"

    - name: Fail if there is not enough free space in the archive dir to create platform backup
      fail:
        msg: >-
           Not enough free space in {{ backup_dir }}. It has {{ available_disk_size }}KiB.
           It needs at least {{ total_platform_size_estimation }}KiB.
      when: available_disk_size|int < total_platform_size_estimation|int

    - name: Estimate remaining space after reserving space for platform backup
      set_fact:
        remaining_disk_size_estimation: "{{ available_disk_size|int - total_platform_size_estimation|int }}"

    - block:
      # Estimate the disk size requirement for the OpenStack backup archive.
      - name: Check the size (in KiB) of directories that will be backed up for openstack
        shell: "du -sh -k  {{ item }} | awk '{print $1}'"
        with_items:
          - "{{ mariadb_dir.path }}"
          - "{{ helm_overrides_sqldump_dir.path }}"
          - "{{ armada_permdir }}/{{ openstack_app_name }}"
          - "{{ helm_charts_permdir }}/starlingx"
        register: size_output_openstack

      # Estimate the OpenStack backup size.
      - name: Estimate the total required disk size for platform openstack archive
        set_fact:
          total_openstack_size_estimation: "{{ total_openstack_size_estimation|default(0)|int + item.stdout|int }}"
        with_items: "{{ size_output_openstack.results }}"
        loop_control:
          label: "{{ item.item }}"

      - name: Fail if there is not enough free space in the archive dir to create openstack backup
        fail:
          msg: >-
             Not enough free space in {{ backup_dir }}.
             Free space available is estimated to {{ remaining_disk_size_estimation }}KiB.
             It needs at least {{ total_openstack_size_estimation }}KiB.
        when: remaining_disk_size_estimation|int < total_openstack_size_estimation|int

      - name: Estimate remaining space after reserving space for openstack backup
        set_fact:
          remaining_disk_size_estimation: "{{ remaining_disk_size_estimation|int - total_openstack_size_estimation|int }}"

      when: check_mariadb_pod.rc == 0 or openstack_status.stdout == "uploaded"

    - name: Create ldap temp dir
      file:
        path: "{{ tempdir.path }}/ldap"
        state: directory
      register: ldap_dir

    - name: Name ldap db backup
      set_fact:
        ldap_db_backup: "{{ ldap_dir.path }}/ldap.db"

    - name: Backup LDAP DB
      command: "slapcat -d 0 -F {{ ldap_schema_path }} -l {{ ldap_db_backup }}"

    - block:
      - name: Create ceph temp dir
        file:
          path: "{{ tempdir.path }}/ceph"
          state: directory
        register: ceph_dir

      - name: Name ceph crushmap backup
        set_fact:
          crushmap_file: "{{ ceph_dir.path }}/crushmap.bin.backup"

      - name: Create ceph crushmap backup
        command: "ceph osd getcrushmap -o {{ crushmap_file }}"

      - name: Register the content of etc/hostname from
        shell: cat /etc/hostname
        register: local_host

      - name: Set local_hostname
        set_fact:
          local_hostname: "{{ local_host.stdout_lines[0] | trim | default('') }}"

      # backup ceph.conf from ctrl-0 if we take backup from ctrl-1 on DX system
      - block:
        - name: Rename ceph.conf backup from controller-0
          set_fact:
            ceph_conf_ctrl_0: "{{ ceph_dir.path }}/ceph_controller-0.conf"

        - name: Copy ceph.conf from controller-0
          command: 'sshpass -p "{{ ansible_become_pass }}" scp -o StrictHostKeyChecking=no
                    sysadmin@controller-0:/etc/ceph/ceph.conf {{ ceph_conf_ctrl_0 }}'
          no_log: true

        when: (system_mode == 'duplex') and (local_hostname == "controller-1")

      when: ceph_backend.stat.exists

    - name: Send application lifecycle notifications for pre-etcd-backup action
      command: /usr/bin/sysinv-utils notify pre-etcd-backup-action
      register: pre_etcd_backup_notification_result
      failed_when: false

    - name: Fail if some application cannot handle the pre-etcd-backup action
      fail:
        msg: >
          Pre-etcd-backup action failed for application
          {{ pre_etcd_backup_notification_result.stderr }}.
      when: pre_etcd_backup_notification_result.rc != 0

    - name: Create etcd snapshot temp dir
      file:
        path: "{{ tempdir.path }}/etcd-snapshot"
        state: directory
      register: etcd_snapshot_dir

    - name: Name etcd snapshot backup
      set_fact:
        etcd_snapshot_file: "{{ etcd_snapshot_dir.path }}/etcd-snapshot.db"

    - name: Get etcd endpoints
      shell: |
        source /etc/platform/openrc
        system addrpool-list --nowrap | awk '/cluster-host-subnet/{print$14}'
      register: etcd_endpoint

    - name: Wrap etcd_endpoint in [] brackets if it's an ipv6 address
      set_fact:
        etcd_endpoint_parsed: "{{ etcd_endpoint.stdout | ipwrap }}"

    - name: Create etcd snapshot
      command: "etcdctl --endpoints https://{{ etcd_endpoint_parsed }}:2379 --cert=/etc/etcd/etcd-client.crt
                --key=/etc/etcd/etcd-client.key --cacert=/etc/etcd/ca.crt snapshot save {{ etcd_snapshot_file }}"
      environment:
        ETCDCTL_API: 3

    - name: Notify applications that etcd-backup succeeded
      command: /usr/bin/sysinv-utils notify post-etcd-backup-action success
      register: post_etcd_backup_notification_result
      failed_when: false

    - name: Fail if there is some other/internal error when sending lifecycle hook.
      fail:
        msg: "Failed to run post-etcd-backup action [{{ post_etcd_backup_notification_result.rc }}]"
      when: post_etcd_backup_notification_result.rc != 0

    - name: Create temp dir for override backup file
      file:
        path: "{{ tempdir.path }}/override"
        state: directory
      register: override_dir

    - name: Name override backup file
      set_fact:
        override_backup_file: "{{ override_dir.path }}/{{ host_override_backup_file }}"

    - name: Create the override backup file
      command: "/usr/bin/sysinv-utils create-host-overrides {{ override_backup_file }}"

    - block:
      - name: Get kubernetes_version from the DB
        shell: echo "select kubeadm_version from kube_cmd_versions" | psql -qAt -d sysinv
        register: kube_ver_select_result
        become_user: postgres

      - name: Set kubernetes_version to the value from DB
        set_fact:
          kubernetes_version: "{{ kube_ver_select_result.stdout_lines[0] }}"

      when: kubernetes_version is not defined

    - name: Get docker registries information
      include_role:
        name: common/push-docker-images
        tasks_from: get_docker_registries

    - name: Append registries configuration
      blockinfile:
        path: "{{ override_backup_file }}"
        marker: ""
        block: "{{ registries|default({}) | to_nice_yaml(indent=2) }}"

    - name: Append kubernetes extra configuration (extraArgs and extraVolumes)
      include_role:
        name: common/prepare-env
        tasks_from: kube-extra-save-configuration
      vars:
        dst_file: "{{ override_backup_file }}"

    - name: Use current timestamp as backups timestamp
      set_fact:
        backup_timestamp: "{{ lookup('pipe', 'date +%Y_%m_%d_%H_%M_%S') }}"

    - name: Attach timestamp to backups filename
      set_fact:
        platform_backup_file: "{{ platform_backup_filename_prefix }}_{{ backup_timestamp }}.tgz"
        docker_local_registry_backup_file: "{{ docker_local_registry_backup_filename_prefix }}_{{ backup_timestamp }}.tgz"
        openstack_backup_file: "{{ openstack_backup_filename_prefix }}_{{ backup_timestamp }}.tgz"
        dc_vault_backup_file: "{{ dc_vault_backup_filename_prefix }}_{{ backup_timestamp }}.tgz"

    - name: Set backup files absolute path
      set_fact:
        platform_backup_file_path: "{{ backup_dir }}/{{ platform_backup_file }}"
        docker_local_registry_backup_file_path: "{{ backup_dir }}/{{ docker_local_registry_backup_file }}"
        openstack_backup_file_path: "{{ backup_dir }}/{{ openstack_backup_file }}"
        dc_vault_backup_file_path: "{{ backup_dir }}/{{ dc_vault_backup_file }}"

    - name: Save user uploaded images from local registry to an archive
      include_tasks: export-user-local-registry-images.yml
      vars:
        export_file_path: "{{ docker_local_registry_backup_file_path }}"
        kilo_free_size: "{{ remaining_disk_size_estimation }}"
      when: should_use_old_image_backup

    - name: Backup registry images filesystem
      include_tasks: backup-image-registry-filesystem.yml
      when: backup_registry_filesystem_required

    - name: Notify applications that backup succeeded
      command: /usr/bin/sysinv-utils notify post-backup-action success
      register: post_backup_notification_result
      failed_when: false

    - name: Fail if there is some other/internal error when sending lifecycle hook.
      fail:
        msg: "Failed to run post-backup action [{{ post_backup_notification_result.rc }}]"
      when: post_backup_notification_result.rc != 0

    # NOTE: Backup contents are defined on roles/backup/backup-system/vars/main.yml
    # Some additional files generated during this playbook's execution are appended to the backup targets here
    - name: Set fact for backup targets with extra files
      set_fact:
        final_backup_targets: "{{ (backup.targets + [
          etcd_snapshot_file,
          helm_overrides_sqldump_dir.path,
          ldap_db_backup,
          override_backup_file,
          postgres_dir.path,
          crushmap_file_,
          ceph_conf_ctrl_0_
        ]) | reject('equalto', '') | list }}"  # rejecting vars that might be empty
      vars:
        crushmap_file_: "{{ crushmap_file | default(\"\") }}"
        ceph_conf_ctrl_0_: "{{ ceph_conf_ctrl_0 | default(\"\") }}"

    - name: Add /etc to backup targets
      set_fact:
        final_backup_targets: "{{ final_backup_targets + etc_backup.targets.full }}"
      when: optimize_etc_backup|bool == false

    - name: Add optimized /etc to backup targets
      set_fact:
        final_backup_targets: "{{ final_backup_targets + etc_backup.targets.optimized }}"
      when: optimize_etc_backup|bool == true

    # This is nasty to understand, but check the -vvv output to see what is going on
    # When we update the community.general collection from 1.3.6 to >=5.2.0, we can use "archive"
    # with exclusion_patterns
    - name: Create a tgz archive for platform backup
      shell: >-
        tar
        --use-compress-program={{ compress_program }}
        --exclude {{ exclude_targets | map('regex_replace', '^/', '') | list | join(' --exclude ') }}
        -cf {{ platform_backup_file_path }}
        $(ls -d
        {{ final_backup_targets | join(' ') }}
        2> /dev/null)
      args:
        warn: false
      # Changing the failed_when behavior to prevent the backup to fail on "file changed as we read it", which
      # makes tar return 1
      register: tar_cmd
      failed_when: tar_cmd.rc >= 2 or tar_cmd.rc < 0

    - name: Create a tgz archive for dc-vault backup
      shell: "tar -czf {{ dc_vault_backup_file_path }} $(ls -d \
           {{ dc_vault_permdir }} 2>/dev/null)"
      args:
        warn: false
      when: check_dc_controller.rc == 0

    - name: Create a tgz archive for OpenStack backup
      shell: "tar -czf {{ openstack_backup_file_path }} $(ls -d \
           {{ armada_permdir }}/{{ openstack_app_name }} \
           {{ helm_charts_permdir }}/starlingx \
           {{ mariadb_dir.path }} \
           {{ helm_overrides_sqldump_dir.path }} 2>/dev/null)"
      args:
        warn: false
      when: check_mariadb_pod.rc == 0 or openstack_status.stdout == "uploaded"

    - name: Notify the user backup tar file(s) are available
      debug:
        msg: >-
          Backup tar file(s) are now available in {{ backup_dir }} on the active controller.

    - block:
      - name: Transfer platform backup tar file to the local machine
        fetch:
          src: "{{ platform_backup_file_path }}"
          dest: "{{ host_backup_dir }}/"
          flat: yes
        no_log: true

      - name: Transfer openstack backup tar files to the local machine if it exists
        fetch:
          src: "{{ openstack_backup_file_path}}"
          dest: "{{ host_backup_dir }}/"
          flat: yes
        when: check_mariadb_pod.rc == 0 or openstack_status.stdout == "uploaded"
        no_log: true

      # TODO transfer docker image archive which may be very big during remote play.
      # Fetch module fills the memory and has a very slow transfer rate due to base64 encoding
      # Maybe use synchronize module after upgrading ansible, backup-restore/transfer-file
      # role shows a github issue regarding why it can't be used now.

      - name: Notify the user where the backup tar file(s) can be found
        debug:
          msg: >-
            Backup tar file(s) have been transferred to {{ host_backup_dir }} on Ansible control host.
      when: inventory_hostname != 'localhost'

  always:
    - name: Remove the temp dir
      file:
        path: "{{ tempdir.path }}"
        state: absent
      when: tempdir is defined and tempdir.path is defined

    - name: Remove the backup in progress flag file
      file:
        path: "{{ backup_in_progress_flag }}"
        state: absent

    - name: Clear backup_in_progress alarm
      script: fm_alarm.py "--clear" "--backup"
      register: alarm_result
      failed_when: false

    - name: Fail if alarm script throws an exception
      fail:
        msg: "Failed to clear backup-in-progress alarm."
      when: alarm_result.rc != 0

  rescue:
    - name: Notify applications that backup failed.
      command: /usr/bin/sysinv-utils notify post-backup-action failure

    - name: Force fail if this playbook or nested playbook failed
      fail:
        msg: "Unable to backup the system. Please check ansible logs for the last failed task."
