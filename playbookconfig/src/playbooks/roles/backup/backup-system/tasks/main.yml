---
#
# Copyright (c) 2019 Wind River Systems, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# ROLE DESCRIPTION:
#   This role is to backup StarlingX platform data and
#   StarlingX OpenStack Application data if the app exists.
#   The backup data are stored in two separate tar files.
#
- name: Do StarlingX backup
  block:
    - name: Generate backup_in_progress alarm
      script: fm_alarm.py "--set" "--backup"
      register: alarm_result
      failed_when: false

    - name: Fail if alarm script throws an exception
      fail:
        msg: "Failed to generate backup-in-progress alarm."
      when: alarm_result.rc != 0

    - name: Create temp dir
      tempfile:
        path: "{{ backup_dir }}"
        state: directory
      register: tempdir

    - name: Create postgres temp dir
      file:
        path: "{{ tempdir.path }}/postgres"
        state: directory
      register: postgres_dir

    - name: Backup roles, table spaces and schemas for databases.
      shell: >-
        sudo -u postgres pg_dumpall
        --clean --schema-only > {{ postgres_dir.path }}/postgres.postgreSql.config
      args:
        warn: false

    - name: Backup postgres, template1, sysinv, barbican, helmv2 db data
      shell: >-
        sudo -u postgres pg_dump --format=plain --inserts --disable-triggers --data-only
        {{ item }} > {{ postgres_dir.path }}/{{ item }}.postgreSql.data
      args:
        warn: false
      with_items:
        - postgres
        - template1
        - sysinv
        - barbican
        - helmv2

    - name: Backup fm db data
      shell: >-
        sudo -u postgres pg_dump --format=plain --inserts --disable-triggers
        --data-only fm --exclude-table=alarm > {{ postgres_dir.path }}/fm.postgreSql.data
      args:
        warn: false

    - name: Backup keystone db data
      shell: >-
        sudo -u postgres pg_dump --format=plain --inserts --disable-triggers
        --data-only keystone > {{ postgres_dir.path }}/keystone.postgreSql.data
      args:
        warn: false

    - name: Check if it is dc controller
      command: >-
        grep -i "distributed_cloud_role\s*=\s*systemcontroller"
        {{ platform_conf_path }}/platform.conf
      register: check_dc_controller
      failed_when: false

    - block:
      - name: Backup dcmanager db for dc controller
        shell: >-
          sudo -u postgres pg_dump --format=plain --inserts --disable-triggers
          --data-only dcmanager > {{ postgres_dir.path }}/dcmanager.postgreSql.data
        args:
          warn: false

      - name: Backup dcorch db for dc controller
        set_fact:
          dcorch_db: "sudo -u postgres pg_dump --format=plain --inserts --disable-triggers --data-only dcorch "

      - name: Update dcorch tables that will be excluded from backup
        set_fact:
          dcorch_db: "{{ dcorch_db }} --exclude-table={{ item }}"
        with_items:
          - orch_job
          - orch_request
          - resource
          - subcloud_resource

      - name: Backup dcorch db
        shell: "{{ dcorch_db }} > {{ postgres_dir.path }}/dcorch.postgreSql.data"

      when: check_dc_controller.rc == 0

    - name: Create mariadb temp dir
      file:
        path: "{{ tempdir.path }}/mariadb"
        state: directory
      register: mariadb_dir

    - name: Check if mariadb pod is running
      shell: >-
        kubectl --kubeconfig={{ kube_config_dir }} get pods -n openstack | grep {{ mariadb_pod }} | grep -i 'running'
      failed_when: false
      register: check_mariadb_pod

    - block:
      - name: Set k8s cmd prefix
        set_fact:
          kube_cmd_prefix: "kubectl --kubeconfig={{ kube_config_dir }} exec -i {{ mariadb_pod }} -n openstack -- bash -c "

      - name: Show databases
        shell: "{{ kube_cmd_prefix }} 'exec mysql -uroot -p\"$MYSQL_DBADMIN_PASSWORD\" -e\"show databases\"'"
        register: databases

      - name: Backup mariadb
        shell: >-
          {{ kube_cmd_prefix }} 'exec mysqldump -uroot -p"$MYSQL_DBADMIN_PASSWORD" {{ item }}' >
          {{ mariadb_dir.path }}/{{ item }}.mariadb.data
        with_items: "{{ databases.stdout_lines | difference(skip_os_dbs) }}"

      when: check_mariadb_pod.rc == 0

    - block:

      - name: Get stx-openstack status
        shell: >-
          source /etc/platform/openrc; system application-show stx-openstack --column status --format value
        failed_when: false
        register: openstack_status

      - name: Fail the backup if MariaDB is not running
        fail:
          msg: "WARNING: {{ mariadb_pod }} is not running. Cannot backup mariadb data."
        when: openstack_status.stdout == "applied"
      when: check_mariadb_pod.rc != 0

    # In order to restore only the stx-openstack, the user needs to manually
    # remove, delete and upload the application.
    # This procedure will make the helm overrides data that is
    # present in the sysinv DB and the stx-openstack DB to be inconsistent.
    # A dump file containing the overrides data is used here to solve this inconsistency issue.
    # Note that this file will contain only 'update' sql commands because the
    # overrides are already created when we uploaded the application.
    - name: Create Helm overrides temp dir
      file:
        path: "{{ tempdir.path }}/helm_overrides_dir"
        state: directory
      register: helm_overrides_dir

    - name: Get the openstack Helm overrides from the from the database
      shell: >-
        psql -c "copy(select row_to_json(t) from (select name, user_overrides, system_overrides
        from helm_overrides where namespace='openstack') as t) to stdout" sysinv | sed -e 's/\\\\/\\/g'
      become_user: postgres
      register: helm_overrides_list

    - name: Generate postgres update commands for Helm overrides
      set_fact:
        updates_list: >
          {{ updates_list | default('') }}update helm_overrides set
          system_overrides={% if item.system_overrides %}'{{ item.system_overrides }}'{% else %}NULL{% endif %},
          user_overrides={% if item.user_overrides %}'{{ item.user_overrides }}'{% else %}NULL{% endif %}
          where name='{{ item.name }}' and namespace='openstack';
      with_items: "{{ helm_overrides_list.stdout_lines | map('from_json') | list }}"

    - name: Backup Helm overrides
      copy:
        dest: "{{ helm_overrides_dir.path }}/helm_overrides_dump.sql"
        mode: 0755
        content: '{{ updates_list | default("") }}'

    # Now Postgres data and MariaDB data are stored in staging dir, we can estimate
    # the disk size requirement for the backup archive.
    - name: Check the size (in KiB) of directories that will be backed up for platform
      shell: "du -sh -k  {{ item }} | awk '{print $1}'"
      with_items:
        - /etc
        - /home
        - "{{ config_permdir }}"
        - "{{ sysinv_permdir }}"
        - "{{ puppet_permdir }}/hieradata"
        - "{{ keyring_permdir }}"
        - "{{ extra_permdir }}"
        - "{{ patching_permdir }}"
        - "{{ patching_repo_permdir }}"
        - "{{ extension_permdir }}"
        - "{{ dc_vault_permdir }}"
        - "{{ deploy_permdir }}"
        - "{{ postgres_dir.path }}"
        - "{{ armada_permdir }}"
        - "{{ helm_charts_permdir }}"
        - "{{ helm_overrides_permdir }}"
        - "{{ helm_overrides_dir.path }}"
      register: size_output_platform

    # Estimate the backup size. We add 128M overhead for things like ceph crushmap,
    # ldap data, etc. that will be generated and stored in the staging dir later on.
    - name: Estimate the total required disk size for platform backup archive
      set_fact:
        total_platform_size_estimation: "{{ total_platform_size_estimation|default(1024*128)|int + item.stdout|int }}"
      with_items: "{{ size_output_platform.results }}"
      loop_control:
        label: "{{ item.item }}"

    - name: Check the free space in the archive dir
      shell: "df -k {{ backup_dir }} --output=avail | tail -1"
      register: df_output

    - name: Parse backup directory size
      set_fact:
        available_disk_size: "{{ df_output.stdout | int }}"

    - name: Fail if there is not enough free space in the archive dir to create platform backup
      fail:
        msg: >-
           Not enough free space in {{ backup_dir }}. It has {{ available_disk_size }}KiB.
           It needs at least {{ total_platform_size_estimation }}KiB.
      when: available_disk_size|int < total_platform_size_estimation|int

    - name: Estimate remaining space after reserving space for platform backup
      set_fact:
        remaining_disk_size_estimation: "{{ available_disk_size|int - total_platform_size_estimation|int }}"

    - block:
      # Estimate the disk size requirement for the openstack backup archive.
      - name: Check the size (in KiB) of directories that will be backed up for openstack
        shell: "du -sh -k  {{ item }} | awk '{print $1}'"
        with_items:
          - "{{ mariadb_dir.path }}"
          - "{{ helm_overrides_dir.path }}"
          - "{{ armada_permdir }}/stx-openstack"
          - "{{ helm_charts_permdir }}/starlingx"
        register: size_output_openstack

      # Estimate the openstack backup size.
      - name: Estimate the total required disk size for platform openstack archive
        set_fact:
          total_openstack_size_estimation: "{{ total_openstack_size_estimation|default(0)|int + item.stdout|int }}"
        with_items: "{{ size_output_openstack.results }}"
        loop_control:
          label: "{{ item.item }}"

      - name: Fail if there is not enough free space in the archive dir to create openstack backup
        fail:
          msg: >-
             Not enough free space in {{ backup_dir }}.
             Free space available is estimated to {{ remaining_disk_size_estimation }}KiB.
             It needs at least {{ total_openstack_size_estimation }}KiB.
        when: remaining_disk_size_estimation|int < total_openstack_size_estimation|int

      - name: Estimate remaining space after reserving space for openstack backup
        set_fact:
          remaining_disk_size_estimation: "{{ remaining_disk_size_estimation|int - total_openstack_size_estimation|int }}"

      when: check_mariadb_pod.rc == 0 or openstack_status.stdout == "uploaded"

    - name: Create ldap temp dir
      file:
        path: "{{ tempdir.path }}/ldap"
        state: directory
      register: ldap_dir

    - name: Name ldap db backup
      set_fact:
        ldap_db_backup: "{{ ldap_dir.path }}/ldap.db"

    - name: Backup ldap db
      command: "slapcat -d 0 -F /etc/openldap/schema -l {{ ldap_db_backup }}"

    - block:
      - name: Create ceph temp dir
        file:
          path: "{{ tempdir.path }}/ceph"
          state: directory
        register: ceph_dir

      - name: Name ceph crushmap backup
        set_fact:
          crushmap_file: "{{ ceph_dir.path }}/crushmap.bin.backup"

      - name: Create ceph crushmap backup
        command: "ceph osd getcrushmap -o {{ crushmap_file }}"

      when: ceph_backend.stat.exists

    - name: Create etcd snapshot temp dir
      file:
        path: "{{ tempdir.path }}/etcd-snapshot"
        state: directory
      register: etcd_snapshot_dir

    - name: Name etcd snapshot backup
      set_fact:
        etcd_snapshot_file: "{{ etcd_snapshot_dir.path }}/etcd-snapshot.db"

    - name: Get etcd endpoints
      shell: |
        source /etc/platform/openrc
        system addrpool-list | awk '/cluster-host-subnet/{print$14}'
      register: etcd_endpoint

    - name: Create etcd snapshot
      command: "etcdctl --endpoints https://{{ etcd_endpoint.stdout }}:2379 --cert=/etc/etcd/etcd-client.crt
                --key=/etc/etcd/etcd-client.key --cacert=/etc/etcd/ca.crt snapshot save {{ etcd_snapshot_file }}"
      environment:
        ETCDCTL_API: 3

    - name: Create temp dir for override backup file
      file:
        path: "{{ tempdir.path }}/override"
        state: directory
      register: override_dir

    - name: Name override backup file
      set_fact:
        override_backup_file: "{{ override_dir.path }}/{{ host_override_backup_file }}"

    - name: Create the override backup file
      command: "/usr/bin/sysinv-utils create-host-overrides {{ override_backup_file }}"

    - name: Get docker registries information
      include_role:
        name: common/push-docker-images
        tasks_from: get_docker_registries

    - name: Append registries configuration
      blockinfile:
        path: "{{ override_backup_file }}"
        marker: ""
        block: "{{ registries|default({}) | to_nice_yaml(indent=2) }}"

    - name: Use current timestamp as backups timestamp
      set_fact:
        backup_timestamp: "{{ lookup('pipe', 'date +%Y_%m_%d_%H_%M_%S') }}"

    - name: Attach timestamp to backups filename
      set_fact:
        platform_backup_file: "{{ platform_backup_filename_prefix }}_{{ backup_timestamp }}.tgz"
        docker_local_registry_backup_file: "{{ docker_local_registry_backup_filename_prefix }}_{{ backup_timestamp }}.tgz"
        openstack_backup_file: "{{ openstack_backup_filename_prefix }}_{{ backup_timestamp }}.tgz"

    - name: Set backup files absolute path
      set_fact:
        platform_backup_file_path: "{{ backup_dir }}/{{ platform_backup_file }}"
        docker_local_registry_backup_file_path: "{{ backup_dir }}/{{ docker_local_registry_backup_file }}"
        openstack_backup_file_path: "{{ backup_dir }}/{{ openstack_backup_file }}"

    - name: Save user uploaded images from local registry to an archive
      import_tasks: export-user-local-registry-images.yml
      vars:
        export_file_path: "{{ docker_local_registry_backup_file_path }}"
        kilo_free_size: "{{ remaining_disk_size_estimation }}"
      when: backup_user_local_registry is defined and backup_user_local_registry|bool == true

    # Archive module has a known bug that doesn't handle empty symbolic links
    # well. Restore to tar command. Can add -P option to keep the leading
    # '/'s in file names in the tar file, so that the tasks that strip leading
    # '/' from the directory names before untar won't be required.
    - name: Create a tgz archive for platform backup
      shell: "tar -czf {{ platform_backup_file_path }} $(ls -d \
           {{ override_backup_file }} \
           /etc \
           /home \
           {{ config_permdir }} \
           {{ sysinv_permdir }} \
           {{ puppet_permdir }}/hieradata \
           {{ keyring_permdir }} \
           {{ extra_permdir }} \
           {{ patching_permdir }} \
           {{ patching_repo_permdir }} \
           {{ extension_permdir }} \
           {{ dc_vault_permdir }} \
           {{ deploy_permdir }} \
           {{ crushmap_file | default(\"\") }} \
           {{ etcd_snapshot_file }} \
           {{ ldap_db_backup }} \
           {{ postgres_dir.path }} \
           {{ armada_permdir }} \
           {{ helm_overrides_permdir }} \
           {{ helm_charts_permdir }} 2>/dev/null)"
      args:
        warn: false

    - name: Create a tgz archive for OpenStack backup
      shell: "tar -czf {{ openstack_backup_file_path }} $(ls -d \
           {{ armada_permdir }}/stx-openstack \
           {{ helm_charts_permdir }}/starlingx \
           {{ mariadb_dir.path }} \
           {{ helm_overrides_dir.path }} 2>/dev/null)"
      args:
        warn: false
      when: check_mariadb_pod.rc == 0 or openstack_status.stdout == "uploaded"

    - name: Notify the user backup tar file(s) are available
      debug:
        msg: >-
          Backup tar file(s) are now available in {{ backup_dir }} on the active controller.

    - block:
      - name: Transfer platform backup tar file to the local machine
        fetch:
          src: "{{ platform_backup_file_path }}"
          dest: "{{ host_backup_dir }}/"
          flat: yes

      - name: Transfer openstack backup tar files to the local machine if it exists
        fetch:
          src: "{{ openstack_backup_file_path}}"
          dest: "{{ host_backup_dir }}/"
          flat: yes
        when: check_mariadb_pod.rc == 0 or openstack_status.stdout == "uploaded"

      # TODO transfer docker image archive which may be very big during remote play.
      # Fetch module fills the memory and has a very slow transfer rate due to base64 encoding
      # Maybe use synchronize module after upgrading ansible, backup-restore/transfer-file
      # role shows a github issue regarding why it can't be used now.

      - name: Notify the user where the backup tar file(s) can be found
        debug:
          msg: >-
            Backup tar file(s) have been transferred to {{ host_backup_dir }} on Ansible control host.
      when: inventory_hostname != 'localhost'

  always:
    - name: Remove the temp dir
      file:
        path: "{{ tempdir.path }}"
        state: absent
      when: tempdir is defined

    - name: Remove the backup in progress flag file
      file:
        path: "{{ backup_in_progress_flag }}"
        state: absent

    - name: Clear backup_in_progress alarm
      script: fm_alarm.py "--clear" "--backup"
      register: alarm_result
      failed_when: false

    - name: Fail if alarm script throws an exception
      fail:
        msg: "Failed to clear backup-in-progress alarm."
      when: alarm_result.rc != 0
