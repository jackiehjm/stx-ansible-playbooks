---
#
# Copyright (c) 2019-2021 Wind River Systems, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# ROLE DESCRIPTION:
#   This role is to bring up Kubernetes and essential flock services required
#   for initial controller unlock.
#

- block:
  - name: Add loopback interface
    # Use shell instead of command module as source is an internal shell command
    shell: "{{ item }}"
    register: add_addresses
    failed_when: false
    with_items:
      - source /etc/platform/openrc; system host-if-add controller-0 lo virtual none lo -c platform -m 1500
      - source /etc/platform/openrc; system interface-network-assign controller-0 lo mgmt
      - source /etc/platform/openrc; system interface-network-assign controller-0 lo cluster-host
      - ip addr add {{ cluster_virtual }}  brd {{ cluster_broadcast }} dev lo scope host label lo:5
      - ip addr add {{ mgmt_virtual }} brd {{ management_broadcast }} dev lo scope host label lo:1
      - ip addr add {{ pxe_virtual }} dev lo scope host
      - ip addr add {{ cluster_floating_virtual }} dev lo scope host
      - ip addr add {{ mgmt_nfs_1_virtual }} dev lo scope host
      - ip addr add {{ mgmt_nfs_2_virtual }} dev lo scope host

  - name: Fail if adding interface addresses failed for reason other than it has been done before
    fail:
      msg: "{{ item.item }} failed for reason: {{ item.stderr }}."
    when: item.rc != 0 and not incomplete_bootstrap
    with_items: "{{ add_addresses.results }}"

  - name: Remove previous management floating address if management network config has changed
    command: ip addr delete {{ prev_mgmt_floating_virtual }} dev lo scope host
    when: last_config_file_exists and reconfigure_endpoints and
          (mgmt_floating_virtual != prev_mgmt_floating_virtual)

  - name: Refresh local DNS (i.e. /etc/hosts)
    import_tasks: refresh_local_dns.yml

  - name: Set up default route to the oam gateway
    include_tasks: setup_default_route.yml

  - name: Copy the central registry certificate
    include_tasks: copy_central_registry_cert.yml
    when: distributed_cloud_role == 'subcloud'

  - name: Load images from archives if configured
    include_tasks: load_images_from_archive.yml
    when: images_archive_exists

  - name: Bring up local docker registry
    import_tasks: bringup_local_registry.yml

  - name: Push images to local docker registry
    import_role:
      name: common/push-docker-images

  - name: Bring up etcd
    systemd:
      name: etcd
      state: started

  - name: Check if etcd-client crt was created.
    find:
      paths: "/etc/etcd"
      patterns: "etcd-client.*"
    register: etcd_client_find_output

  - name: Create etcd client account for root, apiserver and enable etcd auth
    command: "etcdctl --cert-file=$ETCD_CERT --key-file=$ETCD_KEY --ca-file=$ETCD_CA
              --endpoint=$ETCD_ENDPOINT {{ item }}"
    with_items:
      - "user add root:sysadmin"
      - "user add apiserver-etcd-client:sysadmin"
      - "auth enable"
    environment:
      ETCD_ENDPOINT: "https://{{ cluster_floating_address | ipwrap }}:2379"
      ETCD_CERT: "/etc/etcd/etcd-client.crt"
      ETCD_KEY: "/etc/etcd/etcd-client.key"
      ETCD_CA: "/etc/etcd/ca.crt"
    when: etcd_client_find_output.matched != 0

  - name: Bring up Kubernetes master
    import_tasks: bringup_kubemaster.yml

  - name: Bring up Helm
    import_tasks: bringup_helm.yml

  - name: Bring up FluxCD helm and source controllers
    import_tasks: bringup_fluxcd.yml

  - name: Bring up essential flock services
    import_tasks: bringup_flock_services.yml

  - name: Set dnsmasq.leases flag for unlock
    file:
      path: "{{ config_permdir }}/dnsmasq.leases"
      state: touch

  - name: Update resolv.conf file for unlock
    lineinfile:
      path: /etc/resolv.conf
      line: "nameserver {{ controller_floating_address }}"
      insertbefore: BOF

  - name: Check controller-0 is in online state
    shell: source /etc/platform/openrc; system host-show controller-0 --column availability --format value
    register: check_online
    retries: 10
    until: check_online.stdout == "online"

  - name: Wait for {{ pods_wait_time }} seconds to ensure kube-system pods are all started
    wait_for:
      timeout: "{{ pods_wait_time }}"

  - name: Set async parameters
    set_fact:
      async_timeout: 30
      async_retries: 10

  - name: Set Kubernetes components list
    set_fact:
      kube_component_list:
        - k8s-app=calico-node
        - k8s-app=kube-proxy
        - app=multus
        - app=sriov-cni
        - component=kube-apiserver
        - component=kube-controller-manager
        - component=kube-scheduler

  - block:
    - name: Update Kubernetes components list
      set_fact:
        # We skip the calico-node pod on AIO-DX and STANDARD setups
        # because the pods running on a different host than controller-0 will
        # be unreachable at this moment and the calico-node pods
        # will try to connect to them and fail forever
        kube_component_list: >-
         {{ kube_component_list | reject('search', 'calico-node') | list }}

    - name: Get coredns deployment desired replicas
      command: >-
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get deployment
        -n kube-system coredns -o jsonpath={.spec.replicas}
      register: coredns_get_replicas

      # We scale these deployments down and back up because in setups with more
      # than 3 nodes, the cluster could be in the PartialDisruption state and
      # the pods may not be rescheduled off of a down
      # node. This ensures that the pods will be on controller-0 and will
      # become available.
    - name: Scale calico-kube-controllers, armada-api & coredns deployments to 0
      command: >-
        kubectl --kubeconfig=/etc/kubernetes/admin.conf scale deployment
        -n {{ item.namespace }} {{ item.deployment }} --replicas=0
      with_items:
        - { namespace: kube-system, deployment: calico-kube-controllers }
        - { namespace: armada, deployment: armada-api }
        - { namespace: kube-system, deployment: coredns }

    - name: Scale calico-kube-controllers and armada-api deployments back to 1
      command: >-
        kubectl --kubeconfig=/etc/kubernetes/admin.conf scale deployment
        -n {{ item.namespace }} {{ item.deployment }} --replicas=1
      with_items:
        - { namespace: kube-system, deployment: calico-kube-controllers }
        - { namespace: armada, deployment: armada-api }

    - name: Scale coredns deployment back to original size
      command: >-
        kubectl --kubeconfig=/etc/kubernetes/admin.conf scale deployment
        -n kube-system coredns --replicas={{ coredns_get_replicas.stdout }}

    - name: Override async parameters
      set_fact:
        async_timeout: 120
        async_retries: 40

    - name: Wait for 30 seconds to ensure deployments have time to scale back up
      wait_for:
        timeout: 30

    when: mode == 'restore'

  - name: Upgrade k8s networking
    import_role:
      name: common/upgrade-k8s-networking
    when: migrate_platform_data is defined and migrate_platform_data

  - name: Start parallel tasks to wait for Kubernetes component and Networking pods to reach ready state
    # Only check for pods on the current host to avoid waiting for pods on downed nodes
    # This speeds up "Get wait tasks results" on multi-node systems
    command: >-
      kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --namespace=kube-system
      --for=condition=Ready pods --selector {{ item }} --field-selector spec.nodeName=controller-0
      --timeout={{ async_timeout }}s
    async: "{{ async_timeout }}"
    poll: 0
    with_items: "{{ kube_component_list }}"
    register: wait_for_kube_system_pods

  - name: Start wait for armada, fluxcd, calico-kube-controllers & coredns deployments to reach Available state
    # Check the deployment status rather than the pod status in case some pods are down on other nodes
    command: >-
      kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --namespace={{ item.namespace }}
      --for=condition=Available deployment {{ item.deployment }} --timeout={{ async_timeout }}s
    async: "{{ async_timeout }}"
    poll: 0
    with_items:
      - { namespace: kube-system, deployment: calico-kube-controllers }
      - { namespace: flux-helm, deployment: helm-controller }
      - { namespace: flux-helm, deployment: source-controller }
      - { namespace: armada, deployment: armada-api }
      - { namespace: kube-system, deployment: coredns }
    register: wait_for_deployments

  - name: Get wait tasks results
    async_status:
      jid: "{{ item.ansible_job_id }}"
    register: wait_job_result
    until: wait_job_result.finished
    # The retry length should be x2 the length of the async_timeout
    # eg async_retries = async_timeout * 2 / delay
    retries: "{{ async_retries }}"
    delay: 6
    failed_when: false
    with_items:
      - "{{ wait_for_kube_system_pods.results }}"
      - "{{ wait_for_deployments.results }}"

  - name: Fail if any of the Kubernetes component, Networking or Armada pods are not ready by this time
    fail:
      msg: "Pod {{ item.item.item }} is still not ready."
    when: item.stdout is not search(" condition met")
    with_items: "{{ wait_job_result.results }}"

  - name: Enable volume snapshot support
    include_role:
      name: k8s-storage-backends/snapshot-controller
    when: enable_volume_snapshot_support|bool

  when: (not replayed) or (restart_services)
