---
#
# Copyright (c) 2022 Wind River Systems, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#

- hosts: all
  gather_facts: no
  become: yes
  tasks:
    - name: Create /opt/backups
      file:
        path: "/opt/backups"
        state: directory

- hosts: all
  gather_facts: no

  vars_files:
    - vars/common/main.yml
    - host_vars/backup-restore/default.yml

  roles:
    - common/prepare-env
    - common/validate-target
    - backup-restore/validate-input
    - backup-restore/prepare-env
    - backup-restore/stage-backup-archives

- hosts: all
  gather_facts: no
  become: yes

  vars_files:
    - host_vars/backup-restore/default.yml

  tasks:

    - name: Define restore facts
      set_fact:
        hieradata_workdir: /tmp/hieradata
        grub_mkconfig: "{{ 'grub2-mkconfig' if os_release == 'centos' else 'grub-mkconfig' }}"
        network_scripts_location:
          "{{ '/etc/sysconfig/network-scripts' if os_release == 'centos' else '/etc/network/interfaces.d' }}"
        network_scripts_location_bkp:
          "{{ 'etc/sysconfig/network-scripts' if os_release == 'centos' else 'etc/network/interfaces.d' }}"
        docker_registry_service: "{{ 'docker-distribution' if os_release == 'centos' else 'docker-registry' }}"
        root_dir: "{{ '/' if os_release == 'centos' else '/var/rootdirs' }}"
        sysinv_config_permdir: "{{ '/opt/platform/sysinv/' + software_version }}"
        # SSL certs configuration
        ca_cert_dir: "/etc/pki/ca-trust/source/anchors"

    - name: Setup flags to control puppet manifest apply
      file:
        path: "{{ item }}"
        state: touch
      # TODO(abailey): Need to add proper support for duplex
      loop:
        - /etc/platform/simplex

    - name: Create hieradata workdir
      file:
        path: "{{ hieradata_workdir }}"
        state: directory

    - name: Restore puppet hieradata to working directory
      command: "tar -C {{ hieradata_workdir }} -xpf {{ platform_backup_fqpn }} \
                --overwrite --transform='s,.*/,,' \
                opt/platform/puppet/{{ software_version }}/hieradata"
      args:
        warn: false

    - name: Create puppet hieradata runtime configuration
      copy:
        dest: "{{ hieradata_workdir }}/runtime.yaml"
        content: |
          platform::network::mgmt::params::subnet_version: 4
          platform::network::mgmt::params::controller0_address: 127.0.0.1
          platform::network::mgmt::params::controller1_address: 127.0.0.2
        force: yes

    - name: Create SSL CA cert directory
      file:
        path: "{{  ca_cert_dir }}"
        state: directory
        owner: root
        group: root
        mode: 0755
        recurse: yes
      become: yes
      when: os_release == 'debian'

    - name: Applying puppet restore manifest
      command: >-
        /usr/local/bin/puppet-manifest-apply.sh
        {{ hieradata_workdir }}
        localhost
        controller
        restore
        {{ hieradata_workdir }}/runtime.yaml
      environment:
        INITIAL_CONFIG_PRIMARY: "true"
        LC_ALL: "en_US.UTF-8"

    # TODO(outbrito): puppet sets permission to 750, not sure why...
    - name: Set /opt/backups to 755 so postgres can read it
      file:
        path: "/opt/backups"
        state: directory
        mode: 0755

    - name: Create device image filesystem paths
      file:
        path: "{{ item }}"
        state: directory
      loop:
        - /opt/platform/device_images
        - /var/www/pages/device_images

    - name: Create device image bind mount
      command: "mount -o bind -t ext4 /opt/platform/device_images /var/www/pages/device_images"

    - name: Restore configuration files
      command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite {{ item }}"
      loop:
        - etc/barbican
        - etc/containerd
        - etc/default
        - etc/docker
        - etc/docker-distribution
        - etc/drbd.d
        - etc/etcd
        - etc/haproxy
        - etc/hosts
        - etc/keystone
        - etc/kubernetes
        - etc/pki
        - etc/platform/openrc
        - etc/resolv.conf
        - etc/ssl
        - etc/sysinv
      args:
        warn: false

    - name: Update boot loader configuration
      command: "{{ grub_mkconfig }} -o /boot/grub2/grub.cfg"

    - name: Determine network configuration files
      find:
        paths: "{{ network_scripts_location }}"
        patterns: "ifcfg-*"
      register: network_files_to_delete

    - name: Remove network configuration files
      file:
        path: "{{ item.path }}"
        state: absent
      loop: "{{ network_files_to_delete.files }}"

    - name: Restore network configuration files
      command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite --wildcards {{ network_scripts_location_bkp }}/*"

    - name: Restore profile files
      command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite {{ item }}"
      loop:
        - "etc/profile.d/kubeconfig.sh"
      args:
        warn: false

    - name: Restore ldap data
      import_role:
        name: backup-restore/restore-ldap

    - name: Restore etcd snapshot
      import_role:
        name: backup-restore/restore-etcd

    - name: Restore Postgres
      import_role:
        name: backup-restore/restore-postgres

    - name: Restore persistent configuration
      command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite {{ item }}"
      loop:
        - opt/patching
        - opt/platform
        - opt/extension
      args:
        warn: false

    - name: Check archived kubelet dir
      shell: "tar -tf {{ platform_backup_fqpn }} | grep 'var/lib/kubelet'"
      args:
        warn: false
      register: kubelet_dir_result

    - name: Restore kubelet configuration
      command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite var/lib/kubelet/"
      args:
        warn: false
      when: kubelet_dir_result.rc == 0

    - name: Restore kubelet pmond configuration file
      command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite {{ item }}"
      loop:
        - etc/pmon.d/kubelet.conf
      args:
        warn: false

    - name: Reload systemd
      command: systemctl daemon-reload

    - name: Restore container registry filesystem
      command: "tar -C / -xpf {{ registry_backup_fqpn }} --overwrite var/lib/docker-distribution/"
      args:
        warn: false

    - name: Check home dir for CentOS
      block:

        - name: Check if home was backed up
          shell: "tar -tf {{ platform_backup_fqpn }} | grep -E '^home\\/'"
          args:
            warn: false
          register: home_dir_result

        - name: Restore home directory
          command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite home/"
          args:
            warn: false
          when: home_dir_result.rc == 0

      when: os_release == "centos"

    - name: Check home dir for Debian
      block:

        - name: Check if home was backed up
          shell: "tar -tf {{ platform_backup_fqpn }} | grep 'var/home/'"
          args:
            warn: false
          register: home_dir_result

        - name: Restore home directory
          command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite var/home/"
          args:
            warn: false
          when: home_dir_result.rc == 0

      when: os_release == "debian"

    - name: Lookup controller host address
      command: "gethostip -d controller"
      register: host_lookup

    - name: Define controller host address
      set_fact:
        controller_address: "{{ host_lookup.stdout_lines[0] }}"

    - name: Configure controller host address
      command: "ip addr add {{ controller_address }} dev lo scope host"

    - name: Disable local registry authentication
      command: "sed -i '/auth:/,$d' /etc/docker-distribution/registry/config.yml"

    - name: Start docker registry service
      systemd:
        name: "{{ docker_registry_service }}"
        state: restarted

    - name: Start containerd service
      systemd:
        name: containerd
        state: restarted

    - name: Pull kubernetes local container images
      command: "crictl pull registry.local:9001/{{ item }}"
      loop:
        - k8s.gcr.io/kube-apiserver:v1.23.1
        - k8s.gcr.io/kube-scheduler:v1.23.1
        - k8s.gcr.io/kube-controller-manager:v1.23.1
        - k8s.gcr.io/coredns/coredns:v1.8.6

    # restore-more-data/tasks/main.yml#459
    # Set all the hosts including controller-0 to locked/disabled/offline state.
    # After the services are restarted, mtce will update controller-0 to
    # locked/disabled/online state. Setting controller-0 to offline state now
    # will ensure that keystone, sysinv and mtcAgent are indeed in-service after being restated.
    - name: Set all the hosts to locked/disabled/offline state
      shell: >-
        psql -c "update i_host set administrative='locked', operational='disabled',
        availability='offline'" sysinv
      become_user: postgres

    # NOTE(outbrito): If I leave the task below like this, sm comes up as part of the restore and
    # brings drbd up once the node reboots, then I had to enable/start kubelet manually. I also had
    # to bounce drbd since after the snapshot restore, drbd doesn't get the restored data promptly
    # I think there is some kind of caching involved
    - name: Restore complete, set flags
      file:
        path: "{{ item }}"
        state: touch
      loop:
        - /var/run/.ansible_bootstrap  # bootstrap/prepare-env/tasks/main.yml#614
        - /etc/platform/.initial_k8s_config_complete  # bringup_kubemaster.yml#L429
        - /etc/platform/.initial_config_complete  # sm will restart after a while
