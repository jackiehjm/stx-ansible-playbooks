---
#
# Copyright (c) 2022 Wind River Systems, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#

- hosts: all
  gather_facts: no
  become: yes
  tasks:
    - name: Create /opt/backups
      file:
        path: "/opt/backups"
        state: directory

- hosts: all
  gather_facts: no

  vars_files:
    - vars/common/main.yml
    - host_vars/backup-restore/default.yml

  roles:
    - common/prepare-env
    - common/validate-target
    - backup-restore/validate-input
    - backup-restore/prepare-env
    - backup-restore/stage-backup-archives

- hosts: all
  gather_facts: no
  become: yes

  vars_files:
    - host_vars/backup-restore/default.yml
    - vars/backup-restore/main.yml

  tasks:

    - name: Define restore facts
      set_fact:
        hieradata_workdir: /tmp/hieradata
        grub_mkconfig: "{{ 'grub2-mkconfig' if os_release == 'centos' else 'grub-mkconfig' }}"
        network_scripts_location:
          "{{ '/etc/sysconfig/network-scripts' if os_release == 'centos' else '/etc/network/interfaces.d' }}"
        network_scripts_location_bkp:
          "{{ 'etc/sysconfig/network-scripts' if os_release == 'centos' else 'etc/network/interfaces.d' }}"
        docker_registry_service: "{{ 'docker-distribution' if os_release == 'centos' else 'docker-registry' }}"
        root_dir: "{{ '/' if os_release == 'centos' else '/var/rootdirs' }}"
        sysinv_config_permdir: "{{ '/opt/platform/sysinv/' + software_version }}"
        # SSL certs configuration
        ca_cert_dir: "/etc/pki/ca-trust/source/anchors"
        pxelinux_config_permdir: "{{ '/opt/platform/config/' + software_version + '/pxelinux.cfg' }}"

    - name: Setup flags to control puppet manifest apply
      file:
        path: "{{ item }}"
        state: touch
      # TODO(abailey): Need to add proper support for duplex
      loop:
        - /etc/platform/simplex

    - name: Create hieradata workdir
      file:
        path: "{{ hieradata_workdir }}"
        state: directory

    - name: Restore puppet hieradata to working directory
      command: "tar -C {{ hieradata_workdir }} -xpf {{ platform_backup_fqpn }} \
                --overwrite --transform='s,.*/,,' \
                opt/platform/puppet/{{ software_version }}/hieradata"
      args:
        warn: false

    - name: Create puppet hieradata runtime configuration
      copy:
        dest: "{{ hieradata_workdir }}/runtime.yaml"
        content: |
          platform::network::mgmt::params::subnet_version: 4
          platform::network::mgmt::params::controller0_address: 127.0.0.1
          platform::network::mgmt::params::controller1_address: 127.0.0.2
        force: yes

    - name: Create SSL CA cert directory
      file:
        path: "{{  ca_cert_dir }}"
        state: directory
        owner: root
        group: root
        mode: 0755
        recurse: yes
      become: yes
      when: os_release == 'debian'

    - name: Applying puppet restore manifest
      command: >-
        /usr/local/bin/puppet-manifest-apply.sh
        {{ hieradata_workdir }}
        localhost
        controller
        restore
        {{ hieradata_workdir }}/runtime.yaml
      environment:
        INITIAL_CONFIG_PRIMARY: "true"
        LC_ALL: "en_US.UTF-8"

    # TODO(outbrito): puppet sets permission to 750, not sure why...
    - name: Set /opt/backups to 755 so postgres can read it
      file:
        path: "/opt/backups"
        state: directory
        mode: 0755

    - name: Create device image filesystem paths
      file:
        path: "{{ item }}"
        state: directory
      loop:
        - /opt/platform/device_images
        - /var/www/pages/device_images

    - name: Create device image bind mount
      command: "mount -o bind -t ext4 /opt/platform/device_images /var/www/pages/device_images"

    - name: Restore configuration files
      command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite {{ item }}"
      loop:
        - etc/barbican
        - etc/containerd
        - etc/cni
        - etc/default
        - etc/docker
        - etc/docker-distribution
        - etc/drbd.d
        - etc/etcd
        - etc/fm
        - etc/group
        - etc/group-
        - etc/haproxy
        - etc/hosts
        - etc/keystone
        - etc/kubernetes
        - etc/lighttpd
        - etc/mtc
        - etc/mtc.conf
        - etc/mtc.ini
        - etc/passwd
        - etc/passwd-
        - etc/pki
        - etc/platform/openrc
        - etc/profile.d/kubeconfig.sh
        - etc/resolv.conf
        - etc/shadow
        - etc/shadow-
        - etc/sm
        - etc/ssl
        - etc/sysctl.d
        - etc/sysinv
      args:
        warn: false

    - name: Update kernel parameters for iptables
      command: sysctl --system &>/dev/null

    - name: Update boot loader configuration
      command: "{{ grub_mkconfig }} -o /boot/grub2/grub.cfg"

    # Bring up networking, meant to replicate state during boostrapping
    - name: Restore networking
      block:
        - name: Determine network configuration files
          find:
            paths: "{{ network_scripts_location }}"
            patterns: "ifcfg-*"
          register: network_files_to_delete

        - name: Remove network configuration files
          file:
            path: "{{ item.path }}"
            state: absent
          loop: "{{ network_files_to_delete.files }}"

        - name: Restore network configuration files
          command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite --wildcards {{ network_scripts_location_bkp }}/*"

        #fails due to enp0s9 not having the ip set on ifcfg-enp0s9
#        - name: Restart networking daemon
#          systemd:
#            name: networking
#            state: restarted

        - name: Bring lo up
          command: ifup lo lo:1 lo:5

        - name: Lookup controller host address
          command: "gethostip -d controller"
          register: host_lookup

        - name: Define controller host address
          set_fact:
            controller_address: "{{ host_lookup.stdout_lines[0] }}"

        - name: Configure controller host address
          command: "ip addr add {{ controller_address }} dev lo scope host"

        - name: Lookup controller host address
          command: "gethostip -d pxecontroller"
          register: pxe_host_lookup

        - name: Define controller host address
          set_fact:
            pxecontroller_address: "{{ pxe_host_lookup.stdout_lines[0] }}"

        - name: Configure controller host address
          command: "ip addr add {{ pxecontroller_address }} dev lo scope host"

      ignore_errors: true

    - name: Restore Postgres
      import_role:
        name: backup-restore/restore-postgres

    # restore-more-data/tasks/main.yml#459
    # Set all the hosts including controller-0 to locked/disabled/offline state.
    # After the services are restarted, mtce will update controller-0 to
    # locked/disabled/online state. Setting controller-0 to offline state now
    # will ensure that keystone, sysinv and mtcAgent are indeed in-service after being restarted.
    - name: Set all the hosts to locked/disabled/offline state
      shell: >-
        psql -c "update i_host set administrative='locked', operational='disabled',
        availability='offline'" sysinv
      become_user: postgres

    - name: Restore persistent configuration
      command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite {{ item }}"
      loop:
        - opt/patching
        - opt/platform
        - opt/extension
      args:
        warn: false

    - name: Check home dir for CentOS
      block:

        - name: Check if home was backed up
          shell: "tar -tf {{ platform_backup_fqpn }} | grep -E '^home\\/'"
          args:
            warn: false
          register: home_dir_result

        - name: Restore home directory
          command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite home/"
          args:
            warn: false
          when: home_dir_result.rc == 0

      when: os_release == "centos"
    - name: Check home dir for Debian
      block:

        - name: Check if home was backed up
          shell: "tar -tf {{ platform_backup_fqpn }} | grep 'var/home/'"
          args:
            warn: false
          register: home_dir_result

        - name: Restore home directory
          command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite var/home/"
          args:
            warn: false
          when: home_dir_result.rc == 0

      when: os_release == "debian"

    # This shouldn't be needed after restoring /etc/shadow and /etc/passwd. Cache?
    - name: Make sure user sysinv is ready
      user:
        name: sysinv
        group: sysinv
        groups: sys_protected
        shell: /sbin/nologin
        state: present

    - name: Bringup flock services
      systemd:
        name: "{{ item }}"
        state: restarted
      loop:
        - "{{ 'keystone' if os_release == 'debian' else 'openstack-keystone' }}"
        - fminit
        - fm-api
        - sysinv-conductor
        - sysinv-agent
        - sysinv-api
        - mtcClient
        - "{{ 'barbican-api' if os_release == 'debian' else 'openstack-barbican-api' }}"

    - name: Bringup ocf flock services
      command: "{{ item }} start"
      environment:
        OCF_ROOT: "/usr/lib/ocf"
        OCF_RESKEY_state: "active"
      loop:
        - /usr/lib/ocf/resource.d/platform/mtcAgent

    - name: Restore ldap data
      import_role:
        name: backup-restore/restore-ldap

    - name: Restore docker registry
      block:
        - name: Restore container registry filesystem
          command: "tar -C / -xpf {{ registry_backup_fqpn }} --overwrite var/lib/docker-distribution/"
          args:
            warn: false

        - name: Disable local registry authentication
          command: "sed -i '/auth:/,$d' /etc/docker-distribution/registry/config.yml"

        - name: Start docker registry service
          systemd:
            name: "{{ docker_registry_service }}"
            state: restarted

    - name: Restore etcd
      block:
        - name: Restore etcd snapshot
          import_role:
            name: backup-restore/restore-etcd

        - name: Start etcd
          systemd:
            name: etcd
            state: restarted

    - name: Restore kubernetes
      block:
        - name: Start containerd service
          systemd:
            name: containerd
            state: restarted

        - name: Pull kubernetes local container images
          command: "crictl pull registry.local:9001/{{ item }}"
          loop:
            - k8s.gcr.io/kube-apiserver:v1.23.1
            - k8s.gcr.io/kube-scheduler:v1.23.1
            - k8s.gcr.io/kube-controller-manager:v1.23.1
            - k8s.gcr.io/coredns/coredns:v1.8.6

        - name: Check archived kubelet dir
          shell: "tar -tf {{ platform_backup_fqpn }} | grep 'var/lib/kubelet'"
          args:
            warn: false
          register: kubelet_dir_result

        - name: Restore kubelet configuration
          command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite var/lib/kubelet/"
          args:
            warn: false
          when: kubelet_dir_result.rc == 0

        - name: Restore kubelet pmond configuration file
          command: "tar -C / -xpf {{ platform_backup_fqpn }} --overwrite {{ item }}"
          loop:
            - etc/pmon.d/kubelet.conf
          args:
            warn: false

        - name: Get Kubernetes version
          import_role:
            name: common/get-kube-version

        - name: Mount k8s bind mount
          import_role:
            name: common/k8s-bind-mount

        - name: Reload systemd
          command: systemctl daemon-reload

        - name: Start kubelet
          systemd:
            name: kubelet
            state: restarted

    - name: Restore helm service
      block:  # excerpt from bringup_helm.yml
        - name: Ensure helm directories exist
          file:
            path: "{{ item }}"
            state: directory
            recurse: yes
            owner: www
            group: root
          with_items:
            - /var/www/var
            - /var/www/var/log
            - /var/www/tmp

        - name: Create source and target helm bind directories
          file:
            path: "{{ item }}"
            state: directory
            owner: www
            group: root
            mode: 0755
          with_items:
            - "{{ source_helm_bind_dir }}"
            - "{{ target_helm_bind_dir }}"

        - name: Restore Helm charts if the host is bootstrapped in restore mode
          command: tar -C / --overwrite -xpf {{ platform_backup_fqpn }} {{ item }}
          args:
            warn: false
          become_user: root
          with_items:
            - "{{ source_helm_bind_dir | regex_replace('^\\/', '') }}"

        # Note that /opt/platform/helm_charts are owned by www
        # NOTE: helm --debug option displays vital information, no harm enabling.
        # These only show in ansible.log on failure.
        - name: Generate Helm repo indicies
          command: /sbin/helm repo index "{{ source_helm_bind_dir }}/{{ item }}" --debug
          become_user: www
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
            HOME: /home/sysadmin
          with_items:
            - "{{ helm_repo_name_apps }}"
            - "{{ helm_repo_name_platform }}"

        - name: Bind mount on {{ target_helm_bind_dir }}
          # Due to deficiency of mount module, resort to command for now
          command: mount -o bind -t ext4 {{ source_helm_bind_dir }} {{ target_helm_bind_dir }}
          args:
            warn: false

        - name: Enable and Restart lighttpd for Helm
          systemd:
            name: lighttpd
            enabled: yes
            state: restarted

    - name: Create a symlink to PXE config files
      file:
        src: "{{ pxelinux_config_permdir }}"
        dest: /var/pxeboot/pxelinux.cfg
        state: link

    # Make system ready for unlock
    - name: Restore complete, set flags
      file:
        path: "{{ item }}"
        state: touch
      loop:
        - /var/run/.ansible_bootstrap  # bootstrap/prepare-env/tasks/main.yml#614
        - /etc/platform/.initial_k8s_config_complete  # bringup_kubemaster.yml#L429
        - /etc/platform/.initial_config_complete  # sm will restart after a while
